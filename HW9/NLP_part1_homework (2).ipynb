{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Данный ноутбук использовал окружение google-colab\n",
        "%pip install catboost fasttext -q"
      ],
      "metadata": {
        "id": "rwlxK5AYASaT",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b6f8359-0dd7-4bf0-d946-5ad33835f780"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m101.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hOVcvEFxxr1",
        "outputId": "e15793ae-d644-41f1-fa89-342351b20e05"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
            "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting multiprocess<0.70.19 (from datasets)\n",
            "  Downloading multiprocess-0.70.18-py312-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.10.0)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.36.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.3)\n",
            "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Downloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Downloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
            "  Downloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.1/75.1 kB\u001b[0m \u001b[31m960.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
            "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.6/511.6 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.18-py312-none-any.whl (150 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.3/150.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.9/193.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
            "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
            "Downloading frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.6/221.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.3/377.3 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, propcache, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 datasets-4.4.1 dill-0.4.0 frozenlist-1.8.0 multiprocess-0.70.18 propcache-0.4.1 xxhash-3.6.0 yarl-1.22.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание \"NLP. Часть 1\""
      ],
      "metadata": {
        "id": "3xRUXhCVUzur"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "koQiHQFT8XO7"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Tuple, Any\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import datasets\n",
        "import fasttext\n",
        "import fasttext.util\n",
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "ZUhaEvmpTCsv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_pretokenize_text(text: str) -> List[str]:\n",
        "    text = text.lower()\n",
        "    words = re.findall(r'\\b\\w+\\b', text)\n",
        "    return words"
      ],
      "metadata": {
        "id": "_q88wy8uTDZh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This block is for tests only\n",
        "test_corpus = [\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "    \"never jump over the lazy dog quickly\",\n",
        "    \"brown foxes are quick and dogs are lazy\"\n",
        "]\n",
        "\n",
        "def build_vocab(texts: List[str]) -> Tuple[List[str], Dict[str, int]]:\n",
        "    all_words = []\n",
        "    for text in texts:\n",
        "        words = normalize_pretokenize_text(text)\n",
        "        all_words.extend(words)\n",
        "    vocab = sorted(set(all_words))\n",
        "    vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "    return vocab, vocab_index\n",
        "\n",
        "vocab, vocab_index = build_vocab(test_corpus)"
      ],
      "metadata": {
        "id": "uGDzAEpJT_zs"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 1 (0.5 балла)\n",
        "Реализовать One-Hot векторизацию текстов"
      ],
      "metadata": {
        "id": "eemkFZ1tVLw4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_vectorization(\n",
        "    text: str,\n",
        "    vocab: List[str] = None,\n",
        "    vocab_index: Dict[str, int] = None\n",
        ") -> List[List[int]]:\n",
        "    words = normalize_pretokenize_text(text)\n",
        "    result = []\n",
        "\n",
        "    for word in words:\n",
        "        vector = [0] * len(vocab)\n",
        "        if word in vocab_index:\n",
        "            idx = vocab_index[word]\n",
        "            vector[idx] = 1\n",
        "        result.append(vector)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def test_one_hot_vectorization(\n",
        "    vocab: List[str],\n",
        "    vocab_index: Dict[str, int]\n",
        ") -> bool:\n",
        "    try:\n",
        "        text = \"the quick brown fox\"\n",
        "        result = one_hot_vectorization(text, vocab, vocab_index)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result[0]) != expected_length:\n",
        "            return False\n",
        "\n",
        "        words_in_text = normalize_pretokenize_text(text)\n",
        "        for i, word in enumerate(words_in_text):\n",
        "            if word in vocab_index:\n",
        "                idx = vocab_index[word]\n",
        "                if result[i][idx] != 1:\n",
        "                    return False\n",
        "\n",
        "        print(\"One-Hot-Vectors test PASSED\")\n",
        "\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"One-Hot-Vectors test FAILED: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "Qiw7w5OhTDeD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_one_hot_vectorization(vocab, vocab_index)"
      ],
      "metadata": {
        "id": "Q2-LJcmbTe04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ae11d3b-5a86-4adb-a5b0-58441f7b86ae"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-Hot-Vectors test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 2 (0.5 балла)\n",
        "Реализовать Bag-of-Words"
      ],
      "metadata": {
        "id": "hAF8IOYMVT3s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words_vectorization(text: str) -> Dict[str, int]:\n",
        "    words = normalize_pretokenize_text(text)\n",
        "    return dict(Counter(words))\n",
        "\n",
        "def test_bag_of_words_vectorization() -> bool:\n",
        "    try:\n",
        "        text = \"the the quick brown brown brown\"\n",
        "        result = bag_of_words_vectorization(text)\n",
        "\n",
        "        if not isinstance(result, dict):\n",
        "            return False\n",
        "\n",
        "        if result.get('the', 0) != 2:\n",
        "            return False\n",
        "        if result.get('quick', 0) != 1:\n",
        "            return False\n",
        "        if result.get('brown', 0) != 3:\n",
        "            return False\n",
        "        if result.get('nonexistent', 0) != 0:\n",
        "            return False\n",
        "\n",
        "        print(\"Bad-of-Words test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Bag-of-Words test FAILED: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "-_QjiviNBkbS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_bag_of_words_vectorization()"
      ],
      "metadata": {
        "id": "ScFuXh_9TtJm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a3d7ed5-d663-4443-a7fa-d2d68d4812ff"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bad-of-Words test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 3 (0.5 балла)\n",
        "Реализовать TF-IDF"
      ],
      "metadata": {
        "id": "d6LblWJfX2kr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_idf_vectorization(\n",
        "    text: str,\n",
        "    corpus: List[str] = None,\n",
        "    vocab: List[str] = None,\n",
        "    vocab_index: Dict[str, int] = None\n",
        ") -> List[float]:\n",
        "    words = normalize_pretokenize_text(text)\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    total_words = len(words)\n",
        "    tf = {word: count / total_words for word, count in word_counts.items()}\n",
        "\n",
        "    N = len(corpus)\n",
        "    idf = {}\n",
        "\n",
        "    for word in vocab:\n",
        "        df = sum(1 for doc in corpus if word in normalize_pretokenize_text(doc))\n",
        "        if df > 0:\n",
        "            idf[word] = math.log(N / df)\n",
        "        else:\n",
        "            idf[word] = 0.0\n",
        "\n",
        "    result = []\n",
        "    for word in vocab:\n",
        "        if word in tf:\n",
        "            result.append(tf[word] * idf[word])\n",
        "        else:\n",
        "            result.append(0.0)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "def test_tf_idf_vectorization(corpus, vocab, vocab_index) -> bool:\n",
        "    try:\n",
        "        text = \"the quick brown\"\n",
        "        result = tf_idf_vectorization(text, corpus, vocab, vocab_index)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result) != expected_length:\n",
        "            return False\n",
        "\n",
        "        for val in result:\n",
        "            if not isinstance(val, float):\n",
        "                return False\n",
        "\n",
        "        print(\"TF-IDF test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"TF-IDF test FAILED: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "RqcMYJkrTlV0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_tf_idf_vectorization(test_corpus, vocab, vocab_index)"
      ],
      "metadata": {
        "id": "GKIyS724T0XH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f152cff-f83f-423b-9f3b-15d64eb3d2cc"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 4 (1 балл)\n",
        "Реализовать Positive Pointwise Mutual Information (PPMI).  \n",
        "https://en.wikipedia.org/wiki/Pointwise_mutual_information\n",
        "$$PPMI(word, context) = max(0, PMI(word, context))$$\n",
        "$$PMI(word, context) = log \\frac{P(word, context)}{P(word) P(context)} = log \\frac{N(word, context)|(word, context)|}{N(word) N(context)}$$\n",
        "где $N(word, context)$ -- число вхождений слова $word$ в окно $context$ (размер окна -- гиперпараметр)"
      ],
      "metadata": {
        "id": "T0f9FZCrX5_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ppmi_vectorization(\n",
        "    text: str,\n",
        "    corpus: List[str] = None,\n",
        "    vocab: List[str] = None,\n",
        "    vocab_index: Dict[str, int] = None,\n",
        "    window_size: int = 2\n",
        ") -> List[float]:\n",
        "    word_counts = Counter()\n",
        "    cooccurrence_counts = defaultdict(lambda: defaultdict(int))\n",
        "    total_pairs = 0\n",
        "\n",
        "    for doc in corpus:\n",
        "        words = normalize_pretokenize_text(doc)\n",
        "        word_counts.update(words)\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            for j in range(max(0, i - window_size), min(len(words), i + window_size + 1)):\n",
        "                if i != j:\n",
        "                    context_word = words[j]\n",
        "                    cooccurrence_counts[word][context_word] += 1\n",
        "                    total_pairs += 1\n",
        "\n",
        "    total_words = sum(word_counts.values())\n",
        "    words = normalize_pretokenize_text(text)\n",
        "    result = [0.0] * len(vocab)\n",
        "\n",
        "    for word in words:\n",
        "        if word not in word_counts:\n",
        "            continue\n",
        "\n",
        "        p_word = word_counts[word] / total_words\n",
        "\n",
        "        for context_word in vocab:\n",
        "            if context_word not in word_counts:\n",
        "                continue\n",
        "\n",
        "            p_context = word_counts[context_word] / total_words\n",
        "\n",
        "            if total_pairs > 0:\n",
        "                p_joint = cooccurrence_counts[word][context_word] / total_pairs\n",
        "            else:\n",
        "                p_joint = 0\n",
        "\n",
        "            if p_joint > 0 and p_word > 0 and p_context > 0:\n",
        "                pmi = math.log(p_joint / (p_word * p_context))\n",
        "                ppmi = max(0, pmi)\n",
        "            else:\n",
        "                ppmi = 0.0\n",
        "\n",
        "            idx = vocab_index[context_word]\n",
        "            result[idx] += ppmi\n",
        "\n",
        "    return result\n",
        "def test_ppmi_vectorization(corpus, vocab, vocab_index) -> bool:\n",
        "    try:\n",
        "        text = \"quick brown fox\"\n",
        "        result = ppmi_vectorization(text, corpus, vocab, vocab_index)\n",
        "\n",
        "        if not isinstance(result, list):\n",
        "            return False\n",
        "\n",
        "        expected_length = len(vocab)\n",
        "        if len(result) != expected_length:\n",
        "            return False\n",
        "\n",
        "        for val in result:\n",
        "            if not isinstance(val, float):\n",
        "                return False\n",
        "\n",
        "        print(\"PPMI test PASSED\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"PPMI test FAILED: {e}\")\n",
        "        return False"
      ],
      "metadata": {
        "id": "SUg6K2-wTwr6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert test_ppmi_vectorization(test_corpus, vocab, vocab_index)"
      ],
      "metadata": {
        "id": "HgHmNZy75XFV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6106d64-cc90-47f0-ab0c-b337202b40e0"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PPMI test PASSED\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 5 (1 балл)\n",
        "Реализовать получение эмбеддингов из fasttext и bert (для bert лучше использовать CLS токен)"
      ],
      "metadata": {
        "id": "FK29va3PBH_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fasttext_embeddings(\n",
        "    text: str,\n",
        "    model_path: str = None,\n",
        "    model: any = None\n",
        ") -> List[np.ndarray]:\n",
        "    try:\n",
        "        import fasttext\n",
        "        import fasttext.util\n",
        "\n",
        "        # Загружаем модель, если она не передана\n",
        "        if model is None:\n",
        "            if model_path is None:\n",
        "                # Скачиваем модель по умолчанию\n",
        "                fasttext.util.download_model('en', if_exists='ignore')\n",
        "                model = fasttext.load_model('cc.en.300.bin')\n",
        "            else:\n",
        "                model = fasttext.load_model(model_path)\n",
        "\n",
        "        words = normalize_pretokenize_text(text)\n",
        "        embeddings = []\n",
        "\n",
        "        for word in words:\n",
        "            embedding = model.get_word_vector(word)\n",
        "            embeddings.append(embedding)\n",
        "\n",
        "        return embeddings\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in fasttext embeddings: {e}\")\n",
        "        words = normalize_pretokenize_text(text)\n",
        "        return [np.random.randn(300) for _ in words]\n"
      ],
      "metadata": {
        "id": "tOe8dRLl5eqN"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embeddings(\n",
        "    text: str,\n",
        "    model_name: str = 'bert-base-uncased',\n",
        "    pool_method: str = 'cls'\n",
        ") -> np.ndarray:\n",
        "\n",
        "    try:\n",
        "        from transformers import AutoTokenizer, AutoModel\n",
        "        import torch\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            text,\n",
        "            return_tensors='pt',\n",
        "            max_length=512,\n",
        "            truncation=True,\n",
        "            padding=True\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "\n",
        "        if pool_method == 'cls':\n",
        "            embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
        "        elif pool_method == 'mean':\n",
        "            embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
        "        elif pool_method == 'max':\n",
        "            embedding = outputs.last_hidden_state.max(dim=1)[0].squeeze().numpy()\n",
        "        else:\n",
        "            embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n",
        "\n",
        "        return embedding\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in BERT embeddings: {e}\")\n",
        "        return np.random.randn(768)"
      ],
      "metadata": {
        "id": "A9GXy6n0AtsZ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 6 (1.5 балла)\n",
        "Реализовать обучение так, чтобы можно было поверх эмбеддингов, реализованных в предыдущих заданиях, обучить какую-то модель (вероятно неглубокую, например, CatBoost) на задаче классификации текстов ([IMDB](https://huggingface.co/datasets/stanfordnlp/imdb))."
      ],
      "metadata": {
        "id": "E_KoKolrD49R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_dataset(\n",
        "    dataset_name: str = \"imdb\",\n",
        "    vectorizer_type: str = \"bow\",\n",
        "    split: str = \"train\",\n",
        "    sample_size: int = 2500\n",
        ") -> Tuple[Any, List, List]:\n",
        "\n",
        "    print(f\"Loading {split} dataset with {vectorizer_type}...\")\n",
        "\n",
        "    dataset = datasets.load_dataset(dataset_name, split=split)\n",
        "\n",
        "    if sample_size:\n",
        "        indices = []\n",
        "        labels_temp = [item['label'] for item in dataset]\n",
        "\n",
        "        label_0_indices = [i for i, label in enumerate(labels_temp) if label == 0]\n",
        "        label_1_indices = [i for i, label in enumerate(labels_temp) if label == 1]\n",
        "\n",
        "        samples_per_class = min(sample_size // 2, len(label_0_indices), len(label_1_indices))\n",
        "\n",
        "        random.seed(42)\n",
        "        indices.extend(random.sample(label_0_indices, samples_per_class))\n",
        "        indices.extend(random.sample(label_1_indices, samples_per_class))\n",
        "\n",
        "        dataset = dataset.select(indices)\n",
        "\n",
        "    texts = [item['text'] for item in dataset if 'text' in item and item['text'].strip()]\n",
        "    labels = [item['label'] for item in dataset if 'label' in item]\n",
        "\n",
        "    vocab, vocab_index = build_vocab(texts)\n",
        "\n",
        "    vectorized_data = []\n",
        "\n",
        "    for idx, text in enumerate(texts):\n",
        "        if idx % 500 == 0:\n",
        "            print(f\"Processing {idx}/{len(texts)}...\")\n",
        "\n",
        "        try:\n",
        "            if vectorizer_type == \"one_hot\":\n",
        "                one_hot_vecs = one_hot_vectorization(text, vocab, vocab_index)\n",
        "                if one_hot_vecs:\n",
        "                    avg_vec = np.mean(one_hot_vecs, axis=0)\n",
        "                    vectorized_data.append(avg_vec.tolist())\n",
        "                else:\n",
        "                    vectorized_data.append([0] * len(vocab))\n",
        "\n",
        "            elif vectorizer_type == \"bow\":\n",
        "                bow_dict = bag_of_words_vectorization(text)\n",
        "                vector = [bow_dict.get(word, 0) for word in vocab]\n",
        "                vectorized_data.append(vector)\n",
        "\n",
        "            elif vectorizer_type == \"tfidf\":\n",
        "                vectorized_data.append(tf_idf_vectorization(text, texts, vocab, vocab_index))\n",
        "\n",
        "            elif vectorizer_type == \"ppmi\":\n",
        "                vectorized_data.append(ppmi_vectorization(text, texts, vocab, vocab_index))\n",
        "\n",
        "            elif vectorizer_type == \"fasttext\":\n",
        "                embeddings = get_fasttext_embeddings(text)\n",
        "                if embeddings:\n",
        "                    avg_embedding = np.mean(embeddings, axis=0)\n",
        "                    vectorized_data.append(avg_embedding.tolist())\n",
        "                else:\n",
        "                    vectorized_data.append([0] * 300)\n",
        "\n",
        "            elif vectorizer_type == \"bert\":\n",
        "                embedding = get_bert_embeddings(text)\n",
        "                vectorized_data.append(embedding.tolist())\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown vectorizer type: {vectorizer_type}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing text {idx}: {e}\")\n",
        "            if vectorizer_type == \"bert\":\n",
        "                vectorized_data.append([0] * 768)\n",
        "            elif vectorizer_type == \"fasttext\":\n",
        "                vectorized_data.append([0] * 300)\n",
        "            else:\n",
        "                vectorized_data.append([0] * len(vocab))\n",
        "\n",
        "    print(f\"Vectorization complete: {len(vectorized_data)} samples\")\n",
        "    return vocab, vectorized_data, labels"
      ],
      "metadata": {
        "id": "zsc98L8JE8G-"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "\n",
        "def train(\n",
        "    embeddings_method=\"bow\",\n",
        "    test_size=0.2,\n",
        "    val_size=0.2,\n",
        "    cv_folds=5\n",
        "):\n",
        "\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training with {embeddings_method.upper()} embeddings\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "    try:\n",
        "        vocab, X, y = vectorize_dataset(\"imdb\", embeddings_method, \"train\")\n",
        "        _, X_test, y_test = vectorize_dataset(\"imdb\", embeddings_method, \"test\")\n",
        "\n",
        "        X = np.array(X)\n",
        "        y = np.array(y)\n",
        "        X_test = np.array(X_test)\n",
        "        y_test = np.array(y_test)\n",
        "\n",
        "        print(f\"Train shape: {X.shape}, Test shape: {X_test.shape}\")\n",
        "        print(f\"Train labels distribution: {np.bincount(y)}\")\n",
        "        print(f\"Test labels distribution: {np.bincount(y_test)}\")\n",
        "\n",
        "        if len(np.unique(y)) < 2:\n",
        "            print(\"Error: Train set contains only one class!\")\n",
        "            return None\n",
        "\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X, y, test_size=val_size, random_state=42, stratify=y\n",
        "        )\n",
        "\n",
        "        print(f\"Train: {X_train.shape}, Val: {X_val.shape}\")\n",
        "        print(f\"Train val labels distribution: {np.bincount(y_train)}\")\n",
        "        print(f\"Val labels distribution: {np.bincount(y_val)}\")\n",
        "\n",
        "        model = CatBoostClassifier(\n",
        "            iterations=500,\n",
        "            depth=6,\n",
        "            learning_rate=0.1,\n",
        "            loss_function='Logloss',\n",
        "            random_seed=42,\n",
        "            verbose=False,\n",
        "            early_stopping_rounds=50\n",
        "        )\n",
        "\n",
        "        print(\"\\nTraining model...\")\n",
        "        model.fit(\n",
        "            X_train, y_train,\n",
        "            eval_set=(X_val, y_val),\n",
        "            verbose=100\n",
        "        )\n",
        "\n",
        "\n",
        "        y_train_pred = model.predict(X_train)\n",
        "        y_val_pred = model.predict(X_val)\n",
        "        y_test_pred = model.predict(X_test)\n",
        "\n",
        "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "        val_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "        train_f1 = f1_score(y_train, y_train_pred, average='binary')\n",
        "        val_f1 = f1_score(y_val, y_val_pred, average='binary')\n",
        "        test_f1 = f1_score(y_test, y_test_pred, average='binary')\n",
        "\n",
        "        print(f\"Results for {embeddings_method.upper()}\")\n",
        "        print(f\"Train Accuracy: {train_accuracy:.4f} | F1: {train_f1:.4f}\")\n",
        "        print(f\"Val   Accuracy: {val_accuracy:.4f} | F1: {val_f1:.4f}\")\n",
        "        print(f\"Test  Accuracy: {test_accuracy:.4f} | F1: {test_f1:.4f}\")\n",
        "\n",
        "        print(\"Test Set Classification Report:\")\n",
        "        print(classification_report(y_test, y_test_pred, target_names=['Negative', 'Positive']))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during training: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "DRRw01XiBg6H"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for embeddings_method in [\"bow\", \"one_hot\", \"tfidf\", \"ppmi\", \"fasttext\", \"bert\"]:\n",
        "    train(embeddings_method=embeddings_method)"
      ],
      "metadata": {
        "id": "naMqAkjqFHAe",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe26aab-a119-441c-c34c-c590f807ecdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training with BOW embeddings\n",
            "============================================================\n",
            "\n",
            "Loading train dataset with bow...\n",
            "Processing 0/2500...\n",
            "Processing 500/2500...\n",
            "Processing 1000/2500...\n",
            "Processing 1500/2500...\n",
            "Processing 2000/2500...\n",
            "Vectorization complete: 2500 samples\n",
            "Loading test dataset with bow...\n",
            "Processing 0/2500...\n",
            "Processing 500/2500...\n",
            "Processing 1000/2500...\n",
            "Processing 1500/2500...\n",
            "Processing 2000/2500...\n",
            "Vectorization complete: 2500 samples\n",
            "Train shape: (2500, 28274), Test shape: (2500, 28010)\n",
            "Train labels distribution: [1250 1250]\n",
            "Test labels distribution: [1250 1250]\n",
            "Train: (2000, 28274), Val: (500, 28274)\n",
            "Train val labels distribution: [1000 1000]\n",
            "Val labels distribution: [250 250]\n",
            "\n",
            "Training model...\n",
            "0:\tlearn: 0.6795156\ttest: 0.6776975\tbest: 0.6776975 (0)\ttotal: 77.1ms\tremaining: 38.5s\n",
            "100:\tlearn: 0.3289552\ttest: 0.4322608\tbest: 0.4322608 (100)\ttotal: 2.94s\tremaining: 11.6s\n",
            "200:\tlearn: 0.1950970\ttest: 0.3867040\tbest: 0.3867040 (200)\ttotal: 5.79s\tremaining: 8.62s\n",
            "300:\tlearn: 0.1284985\ttest: 0.3669666\tbest: 0.3660941 (290)\ttotal: 8.65s\tremaining: 5.72s\n",
            "400:\tlearn: 0.0912228\ttest: 0.3623383\tbest: 0.3603400 (364)\ttotal: 11.5s\tremaining: 2.83s\n"
          ]
        }
      ]
    }
  ]
}
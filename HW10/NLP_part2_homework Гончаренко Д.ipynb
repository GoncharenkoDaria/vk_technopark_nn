{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tjJocIf2SVzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "275feeaf-ccb7-4732-b12d-2bd0a0ad1938"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_xla/experimental/gru.py:113: SyntaxWarning: invalid escape sequence '\\_'\n",
            "  * **h_n**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{out})` or\n",
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:86: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import warnings\n",
        "import random\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "RtEMuvDVSYBk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание\n",
        "\n",
        "1) Реализовать методы `greedy_sampling` и `generate` (1 балл)\n",
        "2) Реализовать метод `random_sampling` и поддержать его в `generate` (1 балл)\n",
        "3) Реализовать метод `_beam_search_generate` и поддержать его в `generate` (2 балла)\n",
        "4) Реализовать методы `apply_top_p`, `apply_top_k`, `apply_temperature` и поддержать их в `generate` (1 балл)  \n",
        "Все методы необходимо реализовать через векторные операции в torch/numpy везде где это возможно"
      ],
      "metadata": {
        "id": "IvgXC6YxSm12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model:\n",
        "    def __init__(self, model_name: str = \"gpt2\"):\n",
        "        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "        self.model.eval()\n",
        "\n",
        "    def greedy_sampling(self, logits: torch.Tensor) -> int:\n",
        "        return torch.argmax(logits, dim=-1).item()\n",
        "\n",
        "    def random_sampling(self, logits: torch.Tensor) -> int:\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        return torch.multinomial(probs, num_samples=1).item()\n",
        "\n",
        "    def _beam_search_generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int,\n",
        "        num_beams: int\n",
        "    ) -> str:\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')\n",
        "        beams = [(input_ids, 0.0)]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length):\n",
        "                all_candidates = []\n",
        "\n",
        "                for seq, score in beams:\n",
        "                    if seq[0, -1].item() == self.tokenizer.eos_token_id:\n",
        "                        all_candidates.append((seq, score))\n",
        "                        continue\n",
        "\n",
        "                    outputs = self.model(seq)\n",
        "                    logits = outputs.logits[0, -1, :]\n",
        "\n",
        "                    log_probs = F.log_softmax(logits, dim=-1)\n",
        "                    top_log_probs, top_indices = torch.topk(log_probs, num_beams)\n",
        "\n",
        "                    for log_prob, token_id in zip(top_log_probs, top_indices):\n",
        "                        new_seq = torch.cat([seq, token_id.unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "                        new_score = score + log_prob.item()\n",
        "                        all_candidates.append((new_seq, new_score))\n",
        "\n",
        "                beams = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:num_beams]\n",
        "                if all(beam[0][0, -1].item() == self.tokenizer.eos_token_id for beam in beams):\n",
        "                    break\n",
        "\n",
        "        best_seq = beams[0][0]\n",
        "        return self.tokenizer.decode(best_seq[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "    def apply_temperature(self, logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:\n",
        "        return logits / temperature\n",
        "\n",
        "    def _apply_top_k(self, logits: torch.Tensor, top_k: int = 0) -> torch.Tensor:\n",
        "        if top_k <= 0 or top_k >= self.vocab_size:\n",
        "            return logits\n",
        "\n",
        "        # Находим k-й наибольший элемент\n",
        "        top_k_values, _ = torch.topk(logits, top_k)\n",
        "        threshold = top_k_values[-1]\n",
        "\n",
        "        # Маскируем все значения меньше порога\n",
        "        logits_filtered = logits.clone()\n",
        "        logits_filtered[logits < threshold] = float('-inf')\n",
        "\n",
        "        return logits_filtered\n",
        "\n",
        "    def _apply_top_p(self, logits: torch.Tensor, top_p: float = 1.0) -> torch.Tensor:\n",
        "         if top_p >= 1.0:\n",
        "            return logits\n",
        "\n",
        "         sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "\n",
        "         cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "         sorted_indices_to_remove = cumulative_probs > top_p\n",
        "\n",
        "         sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()\n",
        "         sorted_indices_to_remove[0] = False\n",
        "\n",
        "         logits_filtered = logits.clone()\n",
        "         indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "         logits_filtered[indices_to_remove] = float('-inf')\n",
        "\n",
        "         return logits_filtered\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_length: int = 50,\n",
        "        strategy: str = \"greedy\",\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 0,\n",
        "        top_p: float = 1.0,\n",
        "        num_beams: int = 3\n",
        "    ) -> str:\n",
        "\n",
        "        if strategy == \"beam_search\":\n",
        "            return self._beam_search_generate(prompt, max_length, num_beams)\n",
        "\n",
        "        input_ids = self.tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(max_length):\n",
        "                # получаем логиты от модели\n",
        "                outputs = self.model(input_ids)\n",
        "                logits = outputs.logits[0, -1, :]  # (vocab_size,)\n",
        "\n",
        "                # применяем temperature scaling\n",
        "                logits = self.apply_temperature(logits, temperature)\n",
        "\n",
        "                # применяем top-k фильтрацию\n",
        "                if top_k > 0:\n",
        "                    logits = self._apply_top_k(logits, top_k)\n",
        "\n",
        "                # применяем top-p фильтрацию\n",
        "                if top_p < 1.0:\n",
        "                    logits = self._apply_top_p(logits, top_p)\n",
        "\n",
        "                # выбираем следующий токен в зависимости от стратегии\n",
        "                if strategy == \"greedy\":\n",
        "                    next_token_id = self.greedy_sampling(logits)\n",
        "                elif strategy == \"random\":\n",
        "                    next_token_id = self.random_sampling(logits)\n",
        "                else:\n",
        "                    raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "\n",
        "                # добавляем токен к последовательности\n",
        "                input_ids = torch.cat([input_ids, torch.tensor([[next_token_id]])], dim=1)\n",
        "\n",
        "                # останавливаемся при встрече EOS токена\n",
        "                if next_token_id == self.tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "        # декодируем и возвращаем результат\n",
        "        return self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "JToKeNj7SYbx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Продемонстрируйте результат работы `generate` при различных параметрах"
      ],
      "metadata": {
        "id": "aNUHC3UmSYd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(\"gpt2\")\n",
        "prompt = \"once upon a time\"\n",
        "\n",
        "print(\" Greedy Sampling ------------------------\")\n",
        "print(model.generate(prompt, max_length=30, strategy=\"greedy\"))\n",
        "\n",
        "print(\"\\n Random Sampling ------------------------\")\n",
        "print(model.generate(prompt, max_length=30, strategy=\"random\", temperature=0.8))\n",
        "\n",
        "print(\"\\n Random with Top-K---------------------------\")\n",
        "print(model.generate(prompt, max_length=30, strategy=\"random\", top_k=50))\n",
        "\n",
        "print(\"\\n Random with Top-P -----------------------\")\n",
        "print(model.generate(prompt, max_length=30, strategy=\"random\", top_p=0.9))\n",
        "\n",
        "print(\"\\n Beam Search ----------------------\")\n",
        "print(model.generate(prompt, max_length=30, strategy=\"beam_search\", num_beams=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaWd6780N1E_",
        "outputId": "dd380bbe-1598-46e2-e525-553d8af18882"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Greedy Sampling ------------------------\n",
            "once upon a time, and I'm not sure if it's a coincidence or not.\n",
            "\n",
            "I'm not sure if it's a coincidence or not. I'm\n",
            "\n",
            " Random Sampling ------------------------\n",
            "once upon a time?\n",
            "\n",
            "I didn't have an ounce of doubt, I knew it was not a good idea.\n",
            "\n",
            "But I am strong, I look\n",
            "\n",
            " Random with Top-K---------------------------\n",
            "once upon a time it was all a lie. It would be such a good bet to live it up.\"\n",
            "\n",
            "After a couple of weeks of therapy, they had\n",
            "\n",
            " Random with Top-P -----------------------\n",
            "once upon a time, it was when night fell upon my place in myself to defend myself against them. I went off to find my wife and children, and among them\n",
            "\n",
            " Beam Search ----------------------\n",
            "once upon a time.\n",
            "\n",
            "\"I'm not going to lie to you, I'm not going to lie to you, I'm not going to lie to you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"what is in epstein files\"\n",
        "\n",
        "print(\" Greedy Sampling ------------------------\")\n",
        "print(model.generate(prompt, max_length=30, strategy=\"greedy\"))\n",
        "\n",
        "print(\"\\n Random Sampling ------------------------\")\n",
        "print(model.generate(prompt, max_length=30, strategy=\"random\", temperature=0.8))\n",
        "\n",
        "print(\"\\n Random with Top-K---------------------------\")\n",
        "print(model.generate(prompt, max_length=30, strategy=\"random\", top_k=50))\n",
        "\n",
        "print(\"\\n Random with Top-P -----------------------\")\n",
        "print(model.generate(prompt, max_length=30, strategy=\"random\", top_p=0.9))\n",
        "\n",
        "print(\"\\n Beam Search ----------------------\")\n",
        "print(model.generate(prompt, max_length=30, strategy=\"beam_search\", num_beams=5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6G2sWoqOdf2",
        "outputId": "bfad9b29-dc71-4ab9-f5e4-cfdcbd71530f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Greedy Sampling ------------------------\n",
            "what is in epstein files?\n",
            "\n",
            "The epstein file is a collection of files that are stored in the epstein directory. The files are stored in the epstein directory\n",
            "\n",
            " Random Sampling ------------------------\n",
            "what is in epstein files that are not in the archive?)\n",
            "\n",
            "Added NULL to WebObject definitions to avoid naming conflicts with others\n",
            "\n",
            "Added methods for setting height to 100\n",
            "\n",
            " Random with Top-K---------------------------\n",
            "what is in epstein files) :\n",
            "\n",
            "- I just found out the new format that it comes in seems less buggy. I had to fix most tests.\n",
            "\n",
            "-\n",
            "\n",
            " Random with Top-P -----------------------\n",
            "what is in epstein files and what's in sync with HLSF\n",
            "\n",
            "Advantages\n",
            "\n",
            "These compression ratios give a good separation between the image and buffer. I\n",
            "\n",
            " Beam Search ----------------------\n",
            "what is in epstein files?\n",
            "\n",
            "The epstein file is a collection of epstein files. The epstein file is a collection of epstein files. The epstein\n"
          ]
        }
      ]
    }
  ]
}